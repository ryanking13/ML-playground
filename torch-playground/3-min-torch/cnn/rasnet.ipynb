{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "EPOCHS     = 300\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./data',\n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.RandomCrop(32, padding=4),\n",
    "                       transforms.RandomHorizontalFlip(),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                            (0.5, 0.5, 0.5))])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./data',\n",
    "                   train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                            (0.5, 0.5, 0.5))])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Test Loss: 1.4503, Accuracy: 50.71%\n",
      "[2] Test Loss: 1.1001, Accuracy: 61.54%\n",
      "[3] Test Loss: 1.0201, Accuracy: 66.41%\n",
      "[4] Test Loss: 0.9925, Accuracy: 67.03%\n",
      "[5] Test Loss: 0.7590, Accuracy: 73.86%\n",
      "[6] Test Loss: 1.0317, Accuracy: 66.61%\n",
      "[7] Test Loss: 0.7921, Accuracy: 73.23%\n",
      "[8] Test Loss: 0.9361, Accuracy: 69.05%\n",
      "[9] Test Loss: 0.8689, Accuracy: 71.96%\n",
      "[10] Test Loss: 0.7007, Accuracy: 75.83%\n",
      "[11] Test Loss: 0.9491, Accuracy: 70.31%\n",
      "[12] Test Loss: 0.7864, Accuracy: 73.80%\n",
      "[13] Test Loss: 0.7899, Accuracy: 74.80%\n",
      "[14] Test Loss: 0.7438, Accuracy: 75.57%\n",
      "[15] Test Loss: 0.9168, Accuracy: 71.33%\n",
      "[16] Test Loss: 0.8546, Accuracy: 71.74%\n",
      "[17] Test Loss: 0.6582, Accuracy: 78.37%\n",
      "[18] Test Loss: 0.6979, Accuracy: 76.68%\n",
      "[19] Test Loss: 0.7145, Accuracy: 76.89%\n",
      "[20] Test Loss: 0.6839, Accuracy: 77.16%\n",
      "[21] Test Loss: 0.6872, Accuracy: 76.51%\n",
      "[22] Test Loss: 0.8186, Accuracy: 73.55%\n",
      "[23] Test Loss: 0.8937, Accuracy: 71.88%\n",
      "[24] Test Loss: 0.7097, Accuracy: 76.20%\n",
      "[25] Test Loss: 0.8412, Accuracy: 75.06%\n",
      "[26] Test Loss: 0.6371, Accuracy: 79.42%\n",
      "[27] Test Loss: 0.6188, Accuracy: 78.67%\n",
      "[28] Test Loss: 0.7507, Accuracy: 76.67%\n",
      "[29] Test Loss: 0.5474, Accuracy: 81.33%\n",
      "[30] Test Loss: 0.7342, Accuracy: 75.95%\n",
      "[31] Test Loss: 0.7624, Accuracy: 76.35%\n",
      "[32] Test Loss: 0.7801, Accuracy: 75.56%\n",
      "[33] Test Loss: 0.7247, Accuracy: 75.87%\n",
      "[34] Test Loss: 0.7294, Accuracy: 76.50%\n",
      "[35] Test Loss: 0.7122, Accuracy: 77.64%\n",
      "[36] Test Loss: 0.5933, Accuracy: 80.48%\n",
      "[37] Test Loss: 0.6864, Accuracy: 77.15%\n",
      "[38] Test Loss: 0.7240, Accuracy: 76.08%\n",
      "[39] Test Loss: 0.8173, Accuracy: 74.23%\n",
      "[40] Test Loss: 0.7782, Accuracy: 74.62%\n",
      "[41] Test Loss: 0.9032, Accuracy: 72.91%\n",
      "[42] Test Loss: 0.5684, Accuracy: 81.22%\n",
      "[43] Test Loss: 0.9211, Accuracy: 72.71%\n",
      "[44] Test Loss: 0.8377, Accuracy: 72.84%\n",
      "[45] Test Loss: 0.7683, Accuracy: 75.81%\n",
      "[46] Test Loss: 0.7639, Accuracy: 75.60%\n",
      "[47] Test Loss: 0.7023, Accuracy: 76.59%\n",
      "[48] Test Loss: 0.6958, Accuracy: 77.53%\n",
      "[49] Test Loss: 0.6837, Accuracy: 77.67%\n",
      "[50] Test Loss: 0.3566, Accuracy: 87.95%\n",
      "[51] Test Loss: 0.3393, Accuracy: 88.61%\n",
      "[52] Test Loss: 0.3287, Accuracy: 88.88%\n",
      "[53] Test Loss: 0.3285, Accuracy: 88.86%\n",
      "[54] Test Loss: 0.3312, Accuracy: 89.12%\n",
      "[55] Test Loss: 0.3238, Accuracy: 89.24%\n",
      "[56] Test Loss: 0.3194, Accuracy: 89.16%\n",
      "[57] Test Loss: 0.3188, Accuracy: 89.78%\n",
      "[58] Test Loss: 0.3199, Accuracy: 89.35%\n",
      "[59] Test Loss: 0.3460, Accuracy: 88.79%\n",
      "[60] Test Loss: 0.3366, Accuracy: 89.14%\n",
      "[61] Test Loss: 0.3274, Accuracy: 89.20%\n",
      "[62] Test Loss: 0.3291, Accuracy: 89.01%\n",
      "[63] Test Loss: 0.3542, Accuracy: 88.43%\n",
      "[64] Test Loss: 0.3612, Accuracy: 88.54%\n",
      "[65] Test Loss: 0.3502, Accuracy: 88.84%\n",
      "[66] Test Loss: 0.3706, Accuracy: 87.92%\n",
      "[67] Test Loss: 0.3582, Accuracy: 88.50%\n",
      "[68] Test Loss: 0.3430, Accuracy: 89.07%\n",
      "[69] Test Loss: 0.3523, Accuracy: 89.09%\n",
      "[70] Test Loss: 0.3368, Accuracy: 89.33%\n",
      "[71] Test Loss: 0.3302, Accuracy: 89.53%\n",
      "[72] Test Loss: 0.3682, Accuracy: 88.09%\n",
      "[73] Test Loss: 0.3592, Accuracy: 88.70%\n",
      "[74] Test Loss: 0.3447, Accuracy: 89.05%\n",
      "[75] Test Loss: 0.3509, Accuracy: 88.46%\n",
      "[76] Test Loss: 0.3589, Accuracy: 88.71%\n",
      "[77] Test Loss: 0.3475, Accuracy: 89.02%\n",
      "[78] Test Loss: 0.3819, Accuracy: 87.96%\n",
      "[79] Test Loss: 0.3577, Accuracy: 88.65%\n",
      "[80] Test Loss: 0.3888, Accuracy: 87.57%\n",
      "[81] Test Loss: 0.3797, Accuracy: 88.17%\n",
      "[82] Test Loss: 0.3750, Accuracy: 88.28%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    scheduler.step()\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(f'[{epoch}] Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
